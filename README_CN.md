<h2 align="center">Instruction-Guided Fusion of Multi-Layer Visual Features in Large Vision-Language Models</h2>

<div align="center">

[English](README.md) | ç®€ä½“ä¸­æ–‡

</div>

The official implementation of the paper "[Instruction-Guided Fusion of Multi-Layer Visual Features in Large Vision-Language Models](https://arxiv.org/abs/2501.08443)" çš„å®˜æ–¹å®ç°ã€‚


## ğŸ“£ æ–°é—»

- **[Dec 26, 2024]** The paper has been released on [arXiv](https://arxiv.org/abs/2501.08443)å‘å¸ƒï¼
- **[March 10, 202]**  ğŸ”¥ğŸ”¥ğŸ”¥ Code has been released.

## ç›®å½•
- [Performance](#Performance)
- [Framework](#Framework)
- [å®‰è£…](#å®‰è£…)
- [è®­ç»ƒ](#è®­ç»ƒ)
- [è¯„ä¼°](#è¯„ä¼°)

## Performance

<p align="center">
    <img src="images/radar_chart.png" width="70%"></a>
</p>

Performance comparison of our method against the baseline and competing approaches.
+ We systematically analyze how hierarchical visual features influence LVLM
performance across diverse task categories. Our findings reveal that different
layers of the vision encoder play distinct roles, emphasizing the necessity of
task-aware feature fusion rather than static fusion strategies.
+ We propose an instruction-guided vision aggregator, which dynamically assigns
fusion weights to hierarchical visual features based on task-specific instructions.
This mechanism enables LVLMs to selectively emphasize task-relevant features,
improving adaptability without increasing the number of visual tokens.
+ We integrate the proposed vision aggregator into the LLaVA-v1.5 framework,
achieving significant improvements over the baseline and surpassing existing
hierarchical visual feature fusion methods as well as similarly scaled LVLMs

<p align="center">
    <img src="images/mmfuser-diagram.png" width="95%"></a>
</p>



## Framework
<p align="center">
    <img src="images/Framework.png" width="90%"></a>
</p>
The overall framework is illustrated in Fig~\ref{fig:framework}(a). It consists of 4 main modules: a vision encoder $V$, a vision-language adapter $ADP$, an instruction-guided vision aggregator $\mathit{IGVA}$, and an LLM. For demonstration purposes, we use the widely adopted LLaVA-v1.5 \cite{llava1.5} as the implementation framework.

![Overview of the proposed framework.](figure/Figure_4.pdf){#fig:framework}

**Vision Encoder**  
We use CLIP-ViT \cite{clip} as the vision encoder. It divides an input image $\mathbf{I} \in \mathbb{R}^{C \times H \times W}$ into small patches and processes the patch sequence through a stack of transformer layers. The hierarchical output of the vision encoder is:

$$
\mathbf{F} = V(\mathbf{I}) \in \mathbb{R} ^ {L \times (N+1) \times D},
$$

where $L$ is the number of transformer layers, $N$ is the number of patches, and $D$ is the hidden dimension. Each layer produces $N+1$ features, as a `<cls>` token is prepended to the patch sequence to aggregate global information.

---

**Instruction-guided Vision Aggregator**  
The key innovation of our method is the vision aggregator, which dynamically integrates hierarchical visual features based on task instructions. We divide the $L$ layers of the vision encoder into $K$ groups, each containing $L/K$ consecutive layers. For each group, average pooling is applied to the `<cls>` hidden states to obtain a group-wise global feature:

$$
\mathbf{F}^{cls}_{k} = \mathit{Avg}(\mathbf{cls}_{k_1}, \mathbf{cls}_{k_2}, ..., \mathbf{cls}_{k_{L/K}}) \in \mathbb{R}^{ D}, \quad for \ k = 1, 2, ..., K,
$$

where $\mathbf{cls}_{k_i} \in \mathbb{R}^{D}$ is the `<cls>` feature of the i-th layer in group $k$. The aggregator comprises a sentence embedding model and a weight allocator. The sentence embedding model encodes the textual instruction into a semantic embedding $\mathbf{s}$, which is passed to the weight allocator to compute the importance of each visual group:

$$
\mathbf{w} = \mathit{IGVA}(\mathbf{s}, \mathbf{F}^{cls}_{1}, \mathbf{F}^{cls}_{2}, ..., \mathbf{F}^{cls}_{K}) \in \mathbb{R}^{K}, \quad where \sum_{k=1}^K w_k = 1.
$$

For each group, we apply average pooling to the patch features. Then we perform weighted summation across the pooled features using the weight vector $\mathbf{w}$:

$$
\mathbf{F}^{patch}_k = \mathit{Avg}(\mathbf{F}^{patch}_{k_1}, \mathbf{F}^{patch}_{k_2}, ..., \mathbf{F}^{patch}_{k_{L/K}}) \in \mathbb{R} ^ {N \times D}, \quad for \ k = 1, ..., K.
$$

$$
\mathbf{F}_{fused} = \sum_{k=1}^K w_k \times \mathbf{F}^{patch}_k \in \mathbb{R} ^ {N \times D},
$$

where $\mathbf{F}^{patch}_{k_i} \in \mathbb{R}^{N\times D}$ represents the patch features of the i-th layer in group $k$. Finally, to preserve semantic richness, the fused features are concatenated with the penultimate layerâ€™s features to form the final visual representation:

$$
\hat{\mathbf{F}} = \mathit{Concate}(\mathbf{F}_{fused}, \mathbf{F}_{penultimate}) \in \mathbb{R} ^ {N \times 2D}.
$$

---

**Vision-Language Adapter**  
The vision-language adapter aligns the final visual representation with the embedding space of the LLM. It consists of a two-layer MLP with a GELU activation function:

$$
ADP(\hat{\mathbf{F}}) = \mathit{Proj}_2(\mathit{GELU}(\mathit{Proj}_1(\hat{\mathbf{F}}))) \in \mathbb{R} ^ {N \times D_t},
$$

where $D_t$ is the hidden dimension of the L

### é€šç”¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„ç»“æœ

<p align="center">
    <img src="images/eval-leida.png" width="90%"></a>
</p>

ä¸åŒæ¨¡å‹å¤§å°çš„æ€§èƒ½æ¯”è¾ƒã€‚ ï¼ˆå·¦ï¼‰ä¸åŒ…æ‹¬ Qwen-VL-Chatã€LLaVA-1.5-7B åœ¨å†…çš„ 7B æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ 12 ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„ 11 ä¸ªä¸Šå®ç°äº† SoTAã€‚ ï¼ˆå³ï¼‰ä¸åŒ…æ‹¬ InstructBLIPã€LLaVA-1.5-13B åœ¨å†…çš„ 13B æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ 12 ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„ 10 ä¸ªä¸Šå®ç°äº† SoTAã€‚

ä¸ä¼ ç»Ÿ VQA åŸºå‡†æµ‹è¯•å’Œæœ€è¿‘çš„å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ä¸Šæœ€å…ˆè¿›çš„ VLLM è¿›è¡Œæ¯”è¾ƒã€‚æœ€ä½³ç»“æœä»¥ **ç²—ä½“** æ ‡è®°ï¼Œç¬¬äºŒå¥½ç»“æœä»¥ <u>ä¸‹åˆ’çº¿</u> æ ‡è®°ã€‚

<p align="center">
    <img src="images/eval-res.png" width="90%"></a>
</p>

åŠ å…¥MMFuseråï¼ŒLLaVA-1.5çš„æ€§èƒ½å¾—åˆ°äº†å¤§å¹…æå‡ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†LLaVA-1.5ã€‚
å…¶ä¸­ï¼Œåœ¨Vizwizã€MMEå’ŒMMBenchä¸Šçš„å¾—åˆ†åˆ†åˆ«ä¸º57.4ã€
1585.2å’Œ69.9ï¼Œåˆ†åˆ«æ¯”LLaVA-1.5é«˜å‡º3.8åˆ†ã€53.9åˆ†å’Œ2.2åˆ†ã€‚

### OCRBench ä¸Šçš„ç»“æœ

OCRBench æ˜¯ä¸€ä¸ªå…¨é¢çš„ OCR åŸºå‡†ï¼ŒåŒ…å« 1,000 æ¡æ‰‹åŠ¨æ•´ç†å’Œæ ¡æ­£çš„ OCR ç›¸å…³ VQA æŒ‡ä»¤ã€‚å¦‚è¡¨æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹æœ‰ 7B å’Œ 13B ä¸ªå‚æ•°ï¼Œä¸ LLaVA-1.5 ç›¸æ¯”å¹³å‡æé«˜äº† 15 åˆ†ã€‚

<p align="center">
    <img src="images/eval-ocrbench.png" width="55%"></a>
</p>

### åŒºåŸŸçº§åŸºå‡†æµ‹è¯•çš„ç»“æœ

ä¸ºäº†è¯„ä¼°åŒºåŸŸç†è§£å’ŒåŸºç¡€èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„åŒºåŸŸçº§ä»»åŠ¡ä¸Šè¯„ä¼°äº† MMFuserã€‚

1. åŒºåŸŸå­—å¹•ç»“æœ
åœ¨åŒºåŸŸå­—å¹•ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¾ç¤ºå‡ºæ˜¾ç€çš„æ”¹è¿›ã€‚å¦‚è¡¨æ‰€ç¤ºï¼Œä¸ LLaVA-1.5 ç›¸æ¯”ï¼ŒMMFuser çš„ 7B æ¨¡å‹å¹³å‡æ¯” LLaVA-1.5 æé«˜äº† 2.5 åˆ†ï¼Œè€Œ 13B ç‰ˆæœ¬åˆ™æé«˜äº† 3.9 åˆ†ã€‚

2. æŒ‡ç§°è¡¨è¾¾ç†è§£ (REC) çš„ç»“æœ
å¦‚è¡¨æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æŒ‡ç§°è¡¨è¾¾ç†è§£åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äº LLaVA-1.5 æ¨¡å‹ï¼Œä¸ LLaVA-1.5-7B ç›¸æ¯”ï¼Œ7B æ¨¡å‹çš„å¹³å‡æ”¹è¿›å°¤å…¶æ˜¾è‘—ï¼Œä¸º 5.7 åˆ†ã€‚

<p align="center">
    <img src="images/eval-region.png" width="90%"></a>
</p>

### è§†è§‰è¡¨ç°çš„å¯è§†åŒ–

ä¸ºäº†ç›´è§‚åœ°éªŒè¯ MMFuser å¯¹è§†è§‰ç‰¹å¾çš„å½±å“ï¼Œæˆ‘ä»¬åœ¨å›¾ä¸­å±•ç¤ºäº†å››ä¸ªç¤ºä¾‹å›¾åƒçš„è¾“å…¥å’Œè¾“å‡ºç‰¹å¾å›¾å¯è§†åŒ–ã€‚

<p align="center">
    <img src="images/visualization.png" width="90%"></a>
</p>


## å®‰è£…

1. å…‹éš†æ­¤å­˜å‚¨åº“å¹¶åˆ‡æ¢åˆ°MMFuseræ–‡ä»¶å¤¹
    ```bash
    git clone git@github.com:yuecao0119/MMFuser.git
    cd MMFuser
    ```

2. å®‰è£…ç¨‹åºåŒ…

    æˆ‘ä»¬çš„é¡¹ç›®åŸºäº[LLaVA-1.5](https://github.com/haotian-liu/LLaVA)å¹¶æ ¹æ®[LLaVA-1.5å®‰è£…](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#install)åˆ›å»ºç›¸å…³ç¯å¢ƒã€‚

    ```bash
    conda create -n MMFuser python=3.10 -y
    conda activate MMFuser
    pip install --upgrade pip  # enable PEP 660 support
    pip install -e .
    ```

3. å®‰è£…å…¶ä»–è½¯ä»¶åŒ…

    å®‰è£…Flash-Attentionï¼š

    ```bash
    pip install -e ".[train]"
    pip install flash-attn==2.3.6 --no-build-isolation
    ```

    åœ¨æˆ‘ä»¬çš„é¡¹ç›®ä¸­ä½¿ç”¨äº†[Deformation-DETR](https://github.com/fundamentalvision/Deformable-DETR/tree/main)ä¸­çš„å¯å˜å½¢æ³¨æ„åŠ›æœºåˆ¶ã€‚éœ€è¦è¿è¡Œä»¥ä¸‹è„šæœ¬ç¼–è¯‘CUDAç®—å­ï¼š

    ```bash
    cd llava/model/multimodal_projector/deformable_attention/ops
    sh ./make.sh
    # unit test
    python test.py
    ```


## è®­ç»ƒ

æˆ‘ä»¬çš„è®­ç»ƒç®¡çº¿å’Œæ•°æ®é›†ç›´æ¥å–è‡ª[LLaVA-v1.5](https://github.com/haotian-liu/LLaVA). è®­ç»ƒåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼š
- *é¢„è®­ç»ƒ*: åœ¨~558Kå›¾åƒ-æ–‡æœ¬å¯¹çš„å­é›†ä¸Šè®­ç»ƒprojectorï¼Œä»¥è¿æ¥å†»ç»“çš„é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å’Œå†»ç»“çš„å¤§è¯­è¨€æ¨¡å‹ã€‚
    ```bash
    sh scripts/mmfuser/pertrain.sh
    ```
- *æŒ‡ä»¤å¾®è°ƒ*: åˆ©ç”¨å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®LLaVA-665Kå¯¹æ•´ä¸ªMLLMè¿›è¡Œå¾®è°ƒã€‚
    ```bash
    sh scripts/mmfuser/finetune.sh
    ```

## è¯„ä¼°

æˆ‘ä»¬éµå¾ª[LLaVA-v1.5](https://github.com/haotian-liu/LLaVA/tree/main)è¿›è¡Œè¯„ä¼°ã€‚æ‚¨åº”è¯¥ä¸‹è½½[eval.zip](https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view?usp=sharing)ï¼Œå¹¶å°†å…¶è§£å‹ç¼©åˆ°`./playground/data/eval`ã€‚è¯·å‚è€ƒ[Evaluation.md](./docs/Evaluation.md)å‡†å¤‡æ•°æ®ã€‚

ç„¶åï¼Œæ‚¨å¯ä»¥åœ¨`scripts/v1_5/eval`ä¸­è¿è¡Œæˆ‘ä»¬çš„è¯„ä¼°è„šæœ¬ã€‚

å¹¶ä¸”æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç è¿›è¡Œæ¨¡å‹æ¨ç†ï¼š

```bash
sh scripts/mmfuser/inference.sh
```

## ğŸ‘ è‡´è°¢

- [LLaVA](https://github.com/haotian-liu/LLaVA) ï¼šæˆ‘ä»¬åŸºäºè¯¥ä»£ç åº“æ”¹è¿›ã€‚

## ğŸ”’ è®¸å¯è¯

- è¯¥é¡¹ç›®çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½æ˜¯åœ¨[LICENSE](https://github.com/yuecao0119/MMFuser/blob/main/LICENSE)æ–‡ä»¶ä¸­çš„Apache 2.0è®¸å¯è¯ä¸‹å‘å¸ƒçš„ã€‚
- è¯¥æœåŠ¡æ˜¯ä¸€ä¸ªä»…ç”¨äºéå•†ä¸šç”¨é€”çš„ç ”ç©¶é¢„è§ˆï¼Œå—LLaMAçš„[License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)æ¨¡å‹å’ŒOpenAIç”Ÿæˆçš„æ•°æ®çš„[Terms of Use](https://openai.com/policies/terms-of-use)çº¦æŸã€‚å¦‚æœæ‚¨å‘ç°ä»»ä½•æ½œåœ¨çš„è¿è§„è¡Œä¸ºï¼Œè¯·ä¸æˆ‘ä»¬è”ç³»ã€‚

## å¼•ç”¨

å¦‚æœè¿™é¡¹å·¥ä½œå¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä»¥ä¸‹ BibTeX æ¡ç›®ã€‚

```
@article{cao2024mmfuser,
  title={MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding},
  author={Cao, Yue and Liu, Yangzhou and Chen, Zhe and Shi, Guangchen and Wang, Wenhai and Zhao, Danhuai and Lu, Tong},
  journal={arXiv preprint arXiv:2410.11829},
  year={2024}
}
```
